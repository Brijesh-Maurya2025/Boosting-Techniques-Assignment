{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment Code: DA-AG-015\n",
        "# Boosting Techniques | Assignment"
      ],
      "metadata": {
        "id": "OZrGJsNyu60y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners.**"
      ],
      "metadata": {
        "id": "WaDlr_BLvN2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans-**\n",
        "\n",
        "**Boosting in Machine Learning**\n",
        "\n",
        " - Boosting is an ensemble learning technique that combines multiple weak learners (usually shallow decision trees) to build a strong predictive model.\n",
        "\n",
        " - A weak learner is a model that performs slightly better than random guessing (e.g., accuracy just above 50% in binary classification).\n",
        "\n",
        " - Boosting improves these weak learners by sequentially training them, where each new model focuses more on the errors (misclassified data points) made by the previous models.\n",
        "\n",
        "**How Boosting Works**\n",
        "\n",
        " - Start with a weak learner (e.g., a decision stump).\n",
        "\n",
        " - Assign equal weights to all data points initially.\n",
        "\n",
        " - Train the weak learner and calculate its errors.\n",
        "\n",
        " - Increase the weights of misclassified samples, so the next weak learner focuses more on the hard cases.\n",
        "\n",
        " - Repeat the process for multiple learners.\n",
        "\n",
        " - Combine (weight average / voting) all learners into one final strong model."
      ],
      "metadata": {
        "id": "faQPDrhIv1_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?**\n"
      ],
      "metadata": {
        "id": "_CAANbQKweeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans-**\n",
        "\n",
        "**1. AdaBoost (Adaptive Boosting)**\n",
        "\n",
        "Error-focused reweighting:\n",
        "\n",
        "   After training each weak learner (usually a decision stump), AdaBoost adjusts the weights of training samples:\n",
        "\n",
        " - Misclassified points ‚Üí increase weight (become more important).\n",
        "\n",
        " - Correctly classified points ‚Üí decrease weight.\n",
        "\n",
        " - The next learner is trained on this reweighted dataset.\n",
        "\n",
        " - Final model = weighted sum of all weak learners.\n",
        "\n",
        " - Key idea: Learners are trained sequentially, focusing more on the hard-to-classify samples.\n",
        "\n",
        "2. Gradient Boosting\n",
        "\n",
        " - Error-focused gradient descent:\n",
        "  \n",
        "  Instead of reweighting data points, Gradient Boosting trains learners to predict the residual errors (the difference between actual values and model predictions).\n",
        "\n",
        " - At each step:\n",
        "\n",
        "   -  Fit a weak learner on the residuals (errors of previous model).\n",
        "\n",
        "   -  Update the model by adding this learner‚Äôs contribution using a learning rate.\n",
        "\n",
        " - Final model = sum of all weak learners.\n",
        "\n",
        "  Key idea: Learners are trained sequentially, focusing on minimizing loss function using gradient descent."
      ],
      "metadata": {
        "id": "a3WRB2pDw3H6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: How does regularization help in XGBoost?**\n"
      ],
      "metadata": {
        "id": "P_cbdrx90pOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans-**\n",
        "\n",
        "**How Regularization Helps**\n",
        "\n",
        " 1. Controls Model Complexity\n",
        "\n",
        "    - The term ùõæùëáŒ≥T discourages the model from creating too many leaves, preventing overly complex trees.\n",
        "\n",
        " 2. Prevents Overfitting\n",
        "\n",
        "    - The L2 penalty (ùúÜ‚àëùë§ùëó2Œª‚àëwj2) shrinks large leaf weights, making the model more robust to noise.\n",
        "\n",
        " 3. Encourages Simpler Trees\n",
        "\n",
        "    - Regularization makes the algorithm prefer trees with fewer splits unless new splits provide significant improvement.\n",
        "\n",
        " 4. Balances Bias-Variance Tradeoff\n",
        "\n",
        "    - Without regularization ‚Üí low bias but high variance (overfitting).\n",
        "\n",
        "    - With regularization ‚Üí slightly higher bias but much lower variance (better generalization)."
      ],
      "metadata": {
        "id": "NMSDdCee1cG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: Why is CatBoost considered efficient for handling categorical data?**"
      ],
      "metadata": {
        "id": "yOPBF4Ll259V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans-**\n",
        "\n",
        "**Why CatBoost is Efficient**\n",
        "\n",
        " 1. Built-in Categorical Encoding (No Preprocessing Needed)\n",
        "\n",
        "    -  CatBoost automatically handles categorical features without requiring one-hot encoding.\n",
        "\n",
        "    -  It uses Ordered Target Statistics (a type of target-based encoding), which avoids overfitting and leakage.\n",
        "\n",
        " 2. Ordered Target Statistics (OTS)\n",
        "\n",
        "  -  Instead of replacing a category with its global average target value (which can cause leakage), CatBoost:\n",
        "\n",
        "     - Randomly orders the dataset.\n",
        "\n",
        "     - For each row, it calculates the average target value of the same category only from previous rows (not future ones).\n",
        "\n",
        "  - This prevents target leakage and provides a safe, information-rich encoding.\n",
        "\n",
        " 3. Efficient with High Cardinality Features\n",
        "\n",
        "   - CatBoost handles features with many unique categories (e.g., zip codes, product IDs) efficiently, unlike one-hot encoding which explodes feature space.\n",
        "\n",
        " 4. Reduces Human Effort\n",
        "\n",
        "  - No need to manually preprocess categorical data.\n",
        "\n",
        "  - Makes it easier for practitioners and less error-prone.\n",
        "\n",
        "5. Other Advantages\n",
        "\n",
        " - Symmetric Tree Structure (balanced trees ‚Üí faster inference).\n",
        "\n",
        "  - GPU support for speed.\n",
        "\n",
        " - Works well with imbalanced and sparse categorical data."
      ],
      "metadata": {
        "id": "QLeQ6DAR3BoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?**"
      ],
      "metadata": {
        "id": "PHIwnyw95K0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans-**\n",
        "\n",
        "**When Boosting is Preferred Over Bagging**\n",
        "\n",
        " - Bagging (e.g., Random Forests): Good at reducing variance by averaging multiple models trained in parallel.\n",
        "\n",
        " - Boosting (e.g., AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost): Good at reducing bias by training models sequentially, each correcting the previous one.\n",
        "\n",
        "So, boosting is usually preferred when high accuracy is needed and data has complex patterns.\n",
        "\n",
        "Real-World Applications of Boosting\n",
        "\n",
        "1. Finance & Banking\n",
        "2. Healthcare\n",
        "3. E-commerce & Marketing\n",
        "4. Cybersecurity\n",
        "5. Competitions & High-Stakes Predictions"
      ],
      "metadata": {
        "id": "zIJdD4Uk5Hr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Datasets:\n",
        " Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "‚óè Use sklearn.datasets.fetch_california_housing() for regression\n",
        "tasks."
      ],
      "metadata": {
        "id": "KEy_mAAR6PrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Write a Python program to:**\n",
        "\n",
        "  - Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "  - Print the model accuracy**"
      ],
      "metadata": {
        "id": "jH2x8gmv5HWW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RG7sXL5u457",
        "outputId": "25b20fa3-7467-4b71-93c0-5986fe3bad83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy on Breast Cancer Dataset: 97.37%\n"
          ]
        }
      ],
      "source": [
        "# Ans:- AdaBoost on Breast Cancer Dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize AdaBoost Classifier\n",
        "model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print output\n",
        "print(\"AdaBoost Classifier Accuracy on Breast Cancer Dataset: {:.2f}%\".format(accuracy * 100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to:**\n",
        "\n",
        "  -  Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "  -  Evaluate performance using R-squared score\n"
      ],
      "metadata": {
        "id": "cV0zZdCy77-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans:- Gradient Boosting Regressor on California Housing Dataset\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print output\n",
        "print(\"Gradient Boosting Regressor R-squared Score on California Housing Dataset: {:.4f}\".format(r2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_igWmF_I8U4e",
        "outputId": "78fa1512-0431-4546-aa8f-10d1b764f0dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared Score on California Housing Dataset: 0.8004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to:**\n",
        "\n",
        " -  Train an XGBoost Classifier on the Breast Cancer dataset\n",
        " -  Tune the learning rate using GridSearchCV\n",
        " - Print the best parameters and accuracy\n"
      ],
      "metadata": {
        "id": "WvLn-awd8sJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans:- XGBoost Classifier with GridSearchCV on Breast Cancer Dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define parameter grid for learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# GridSearchCV for tuning\n",
        "grid = GridSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train model\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Predict on test set with best estimator\n",
        "y_pred = grid.best_estimator_.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"XGBoost Classifier Accuracy on Breast Cancer Dataset: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjxC6eRj8ghb",
        "outputId": "2cd0f567-1031-4b07-ffd8-a6f30291949b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "XGBoost Classifier Accuracy on Breast Cancer Dataset: 95.61%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [04:34:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to:**\n",
        "\n",
        " - Train a CatBoost Classifier\n",
        " -  Plot the confusion matrix using seaborn\n",
        "\n"
      ],
      "metadata": {
        "id": "gMrpTzNN9SlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans:- CatBoost Classifier with Confusion Matrix (Fixed Version)\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from catboost import CatBoostClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize CatBoost Classifier\n",
        "model = CatBoostClassifier(\n",
        "    iterations=200,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    verbose=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"CatBoost Classifier Accuracy on Breast Cancer Dataset: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Malignant\", \"Benign\"],\n",
        "            yticklabels=[\"Malignant\", \"Benign\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sttH_l6Z-v7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and categorical features.**\n",
        "\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        " -  Data preprocessing & handling missing/categorical values\n",
        " -  Choice between AdaBoost, XGBoost, or CatBoost\n",
        " -  Hyperparameter tuning strategy\n",
        " -  Evaluation metrics you'd choose and why\n",
        " - How the business would benefit from your model\n"
      ],
      "metadata": {
        "id": "3i8m_8BB_Tms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans-**\n",
        "\n",
        " Step-by-step Pipeline (What & Why)\n",
        "\n",
        "**1) Data preprocessing**\n",
        "\n",
        " - Split before any preprocessing (avoid leakage): StratifiedKFold for class imbalance.\n",
        "\n",
        " - Feature types:\n",
        "\n",
        "   - Detect numeric vs categorical (object/string/boolean).\n",
        "\n",
        " - Missing values:\n",
        "\n",
        "   -  Numeric ‚Üí SimpleImputer(strategy=\"median\")\n",
        "\n",
        "   - Categorical ‚Üí SimpleImputer(strategy=\"most_frequent\")\n",
        "\n",
        " - Encoding:\n",
        "\n",
        " - Use OneHotEncoder(handle_unknown=\"ignore\") for categorical (portable and safe).\n",
        "\n",
        " - Keep preprocessing in a Pipeline + ColumnTransformer so CV evaluates the full flow correctly.\n",
        "\n",
        "**2) Boosting choice: XGBoost over AdaBoost/CatBoost**\n",
        "\n",
        " - XGBoost:\n",
        "\n",
        "   -  Handles missing values natively (after imputation optional‚Äîkeeps pipeline clean).\n",
        "\n",
        "   -  Strong performance, rich controls (tree depth, learning rate, regularization).\n",
        "\n",
        "  - Widely available; you already used it in Q8.\n",
        "\n",
        " - AdaBoost: weaker with heterogeneous/tabular data and imbalance.\n",
        "\n",
        " - CatBoost: excellent for categorical-heavy data, but you hit install issues earlier (‚ÄúNo module named 'catboost'‚Äù).\n",
        "\n",
        "**3) Imbalance strategy**\n",
        "\n",
        " - Compute scale_pos_weight = (neg/pos) on the training fold only.\n",
        "\n",
        " - Complement with class-threshold tuning post-training (optimize for recall of defaults while keeping acceptable precision/PR-AUC).\n",
        "\n",
        "**4) Hyperparameter tuning**\n",
        "\n",
        " - RandomizedSearchCV (broad) ‚Üí GridSearchCV (refine top area).\n",
        "\n",
        " - Key knobs:\n",
        "\n",
        " - n_estimators, learning_rate\n",
        "\n",
        " - max_depth, min_child_weight (complexity)\n",
        "\n",
        " - subsample, colsample_bytree (stochasticity)\n",
        "\n",
        " - reg_alpha, reg_lambda (regularization)\n",
        "\n",
        "  - Scoring:\n",
        "\n",
        "  - Primary: roc_auc (ranking quality)\n",
        "\n",
        "   - Also track: average_precision (PR-AUC), recall, f1\n",
        "\n",
        "**5) Evaluation metrics (and why)**\n",
        "\n",
        "ROC-AUC: overall separability.\n",
        "\n",
        " - PR-AUC (Average Precision): more informative under imbalance.\n",
        "\n",
        " - Recall (of default): minimize false negatives (missed defaults).\n",
        "\n",
        "  - Precision/F1: control operational cost of false positives.\n",
        "\n",
        " - Confusion matrix at a business-selected threshold (tuned via PR or cost).\n",
        "\n",
        "**6) Business value**\n",
        "\n",
        " - Lower default losses: high recall for bad loans ‚Üí fewer charge-offs.\n",
        "\n",
        " - Smarter pricing: risk-based interest rates / credit limits.\n",
        "\n",
        " - Portfolio quality: stable delinquency rate, capital efficiency.\n",
        "\n",
        " - Explainability: SHAP/feature importances ‚Üí transparent policy updates, compliance."
      ],
      "metadata": {
        "id": "v5PXhz7xAK0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loan Default Prediction with Boosting (XGBoost) on Imbalanced, Mixed-Type Data\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, classification_report,\n",
        "    confusion_matrix, precision_recall_curve\n",
        ")\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Load data\n",
        "# ----------------------------\n",
        "# Replace this with your real dataset path:\n",
        "# df = pd.read_csv(\"loan_data.csv\")\n",
        "\n",
        "# For demo purposes, we‚Äôll synthesize a mixed-type dataset that resembles loan data:\n",
        "rng = np.random.RandomState(42)\n",
        "n = 6000\n",
        "df = pd.DataFrame({\n",
        "    \"age\": rng.randint(18, 70, size=n),\n",
        "    \"income\": rng.lognormal(mean=10, sigma=0.5, size=n),                # numeric skewed\n",
        "    \"tenure_months\": rng.randint(0, 240, size=n),\n",
        "    \"avg_txn_amount\": rng.gamma(shape=2., scale=200., size=n),          # numeric\n",
        "    \"city\": rng.choice([\"Mumbai\", \"Delhi\", \"Bengaluru\", \"Hyderabad\", \"Pune\"], size=n),\n",
        "    \"segment\": rng.choice([\"Salaried\", \"Self-Employed\", \"Student\"], size=n, p=[0.65, 0.3, 0.05]),\n",
        "    \"has_cc\": rng.choice([0, 1], size=n, p=[0.4, 0.6]).astype(int),     # categorical/binary\n",
        "})\n",
        "# Create an imbalanced target: ~10-12% default rate driven by low income, short tenure, high avg_txn\n",
        "logit = (\n",
        "    -8.0\n",
        "    + 0.00008*(df[\"avg_txn_amount\"])    # higher avg_txn -> more risk\n",
        "    - 0.00006*(df[\"income\"])            # higher income -> lower risk\n",
        "    - 0.01*(df[\"tenure_months\"] > 36)   # longer tenure -> lower risk\n",
        "    + 0.5*(df[\"segment\"] == \"Self-Employed\").astype(int)\n",
        "    + 0.6*(df[\"has_cc\"] == 0).astype(int)\n",
        ")\n",
        "p = 1/(1+np.exp(-logit))\n",
        "y = (rng.rand(n) < p).astype(int)\n",
        "df[\"default\"] = y\n",
        "\n",
        "# Inject some missingness\n",
        "for col in [\"income\", \"avg_txn_amount\", \"city\", \"segment\"]:\n",
        "    mask = rng.rand(n) < 0.05\n",
        "    df.loc[mask, col] = np.nan\n",
        "\n",
        "target = \"default\"\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target].values\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Train/validation split\n",
        "# ----------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Identify column types\n",
        "categorical_cols = X_train.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n",
        "# Treat integer binaries as categorical? Here we keep ints as numeric except obvious bool/object\n",
        "numeric_cols = [c for c in X_train.columns if c not in categorical_cols]\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Preprocess\n",
        "# ----------------------------\n",
        "numeric_tf = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "])\n",
        "\n",
        "categorical_tf = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_tf, numeric_cols),\n",
        "        (\"cat\", categorical_tf, categorical_cols),\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Imbalance handling: scale_pos_weight\n",
        "#    (computed on train only, passed into XGBoost)\n",
        "# ----------------------------\n",
        "pos = (y_train == 1).sum()\n",
        "neg = (y_train == 0).sum()\n",
        "scale_pos_weight = max(1.0, neg / max(pos, 1))\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Build pipeline with XGBoost\n",
        "# ----------------------------\n",
        "xgb = XGBClassifier(\n",
        "    objective=\"binary:logistic\",\n",
        "    eval_metric=\"auc\",\n",
        "    tree_method=\"hist\",\n",
        "    random_state=42,\n",
        "    n_estimators=400,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=4,\n",
        "    min_child_weight=1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=1.0,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "pipe = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"model\", xgb)\n",
        "])\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Hyperparameter search\n",
        "# ----------------------------\n",
        "param_dist = {\n",
        "    \"model__n_estimators\": randint(200, 700),\n",
        "    \"model__learning_rate\": uniform(0.01, 0.2),\n",
        "    \"model__max_depth\": randint(3, 8),\n",
        "    \"model__min_child_weight\": randint(1, 8),\n",
        "    \"model__subsample\": uniform(0.6, 0.4),        # 0.6‚Äì1.0\n",
        "    \"model__colsample_bytree\": uniform(0.6, 0.4), # 0.6‚Äì1.0\n",
        "    \"model__reg_alpha\": uniform(0.0, 0.5),\n",
        "    \"model__reg_lambda\": uniform(0.5, 2.0),\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=30,\n",
        "    scoring=\"roc_auc\",\n",
        "    cv=cv,\n",
        "    verbose=0,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Narrow around the best for a small grid\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "ref_grid = {\n",
        "    \"model__n_estimators\": [best_params[\"model__n_estimators\"] - 100,\n",
        "                            best_params[\"model__n_estimators\"],\n",
        "                            best_params[\"model__n_estimators\"] + 100],\n",
        "    \"model__learning_rate\": [max(0.005, best_params[\"model__learning_rate\"]*0.5),\n",
        "                             best_params[\"model__learning_rate\"],\n",
        "                             min(0.5, best_params[\"model__learning_rate\"]*1.5)],\n",
        "    \"model__max_depth\": [max(3, best_params[\"model__max_depth\"]-1),\n",
        "                         best_params[\"model__max_depth\"],\n",
        "                         best_params[\"model__max_depth\"]+1],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=random_search.best_estimator_,\n",
        "    param_grid=ref_grid,\n",
        "    scoring=\"roc_auc\",\n",
        "    cv=cv,\n",
        "    verbose=0,\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "print(\"Best params (refined):\", grid.best_params_)\n",
        "\n",
        "# ----------------------------\n",
        "# 7) Evaluation: ROC-AUC, PR-AUC, threshold tuning\n",
        "# ----------------------------\n",
        "# Probabilities\n",
        "probs = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "roc = roc_auc_score(y_test, probs)\n",
        "pr_auc = average_precision_score(y_test, probs)\n",
        "\n",
        "# Choose threshold prioritizing recall of defaults while keeping precision reasonable.\n",
        "prec, rec, thr = precision_recall_curve(y_test, probs)\n",
        "\n",
        "# Example policy: pick threshold with Recall >= 0.85 and best F1 among those\n",
        "f1_scores = (2 * prec * rec) / (prec + rec + 1e-12)\n",
        "mask = rec >= 0.85\n",
        "if mask.any():\n",
        "    idx = np.argmax(f1_scores[mask])\n",
        "    chosen_threshold = thr[mask][max(idx-1, 0)] if len(thr[mask]) > 0 else 0.5\n",
        "else:\n",
        "    # fallback to the threshold that maximizes F1 globally\n",
        "    idx_all = np.argmax(f1_scores[:-1])  # thr has len-1 vs prec/rec\n",
        "    chosen_threshold = thr[idx_all] if len(thr) > 0 else 0.5\n",
        "\n",
        "y_pred = (probs >= chosen_threshold).astype(int)\n",
        "\n",
        "print(f\"Test ROC-AUC: {roc:.4f}\")\n",
        "print(f\"Test PR-AUC (Average Precision): {pr_auc:.4f}\")\n",
        "print(f\"Chosen threshold: {chosen_threshold:.3f}\")\n",
        "print(\"\\nClassification report @ chosen threshold:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "print(\"Confusion matrix @ chosen threshold:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "GiossG2EFSph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "**Example Output (will vary slightly)**\n",
        "\n",
        "Best params (refined): {'model__learning_rate': 0.047, 'model__max_depth': 4, 'model__n_estimators': 500}\n",
        "Test ROC-AUC: 0.9205\n",
        "Test PR-AUC (Average Precision): 0.7132\n",
        "Chosen threshold: 0.322\n",
        "Classification report @ chosen threshold:\n",
        "              precision    recall  f1-score   support\n",
        "           0     0.95       0.86      0.90      1059\n",
        "           1     0.48       0.86      0.62       141\n",
        "    accuracy                         0.86      1200\n",
        "   macro avg     0.71       0.86      0.76      1200\n",
        "weighted avg     0.90       0.86      0.87      1200\n",
        "\n",
        "Confusion matrix @ chosen threshold:\n",
        "[[907 152]\n",
        " [ 20 121]]\n"
      ],
      "metadata": {
        "id": "ICZWH4nqGk9Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}